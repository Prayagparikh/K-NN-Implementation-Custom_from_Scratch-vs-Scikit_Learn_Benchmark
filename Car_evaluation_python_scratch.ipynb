{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "894d8618-9b97-4ca6-bd09-f0364be52ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from math import sqrt\n",
    "import random\n",
    "import copy\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4201d2f-64c9-4f0b-95dd-090b629d7e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0      1  2  3      4     5      6\n",
      "0  vhigh  vhigh  2  2  small   low  unacc\n",
      "1  vhigh  vhigh  2  2  small   med  unacc\n",
      "2  vhigh  vhigh  2  2  small  high  unacc\n",
      "3  vhigh  vhigh  2  2    med   low  unacc\n",
      "4  vhigh  vhigh  2  2    med   med  unacc\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"./car+evaluation/car.data.csv\", header=None)\n",
    "dataset = dataset.mask(dataset == '')\n",
    "print(dataset.head(5))\n",
    "dataset = dataset.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3f7e044-08b2-4330-b867-25bc51691f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows are Mapping:\n",
      "[0, 0, 1, 0, 0, 2, 2]\n",
      "[0, 0, 1, 0, 0, 1, 2]\n",
      "[0, 0, 1, 0, 0, 0, 2]\n",
      "[0, 0, 1, 0, 2, 2, 2]\n",
      "[0, 0, 1, 0, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Converting STRING values to unique int values for each column:\n",
    "# buying:   vhigh, high, med, low. -> (0,2,3,1)\n",
    "# maint:    vhigh, high, med, low. -> (0,2,3,1)\n",
    "# doors:    2, 3, 4, 5more -> (1, 0, 2, 3)\n",
    "# persons:  2, 4, more -> (1, 2, 0)\n",
    "# lug_boot: small, med, big -> (0, 2, 1)\n",
    "# safety:   low, med, high -> (2, 0, 1)\n",
    "# class: good, vgood, acc, unacc -> (0, 1 ,2, 3) respectively\n",
    "# Each of above column will\n",
    "\n",
    "def str_column_to_int(dataset, column):\n",
    "    class_values = [row[column] for row in dataset]\n",
    "    unique = set(class_values)\n",
    "    lookup = {value: index for index, value in enumerate(unique)}\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    return lookup\n",
    "\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_int(dataset, i)\n",
    "\n",
    "print(\"First 5 rows are Mapping:\")\n",
    "for i in range(5):\n",
    "    print(dataset[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f67938-e6df-49ca-8ce1-633c145338c0",
   "metadata": {},
   "source": [
    "## Full Scratch KNN model: For Car_Evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bad999b1-889d-4a59-9468-894dcab396b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************ FINAL PREDICTION OUTPUT ************************\n",
      "New Observation: [3, 0, 3, 2, 2, 0]\n",
      "Predicted class(k=3): good\n",
      "Predicted class(k=sqrt(dataset_size)): good\n"
     ]
    }
   ],
   "source": [
    "# PLEASE REFER: Details regarding Hayes Roth Dataset and how it is used here is in 'Doc file'\n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "    distance = 0.0\n",
    "    for i in range(len(row2)-2):\n",
    "        distance += (row1[i] - row2[i])**2\n",
    "    return sqrt(distance)\n",
    "\n",
    "def get_neighbors(X_train, test_row, k):\n",
    "    distance = list()\n",
    "    data = []\n",
    "    for i in X_train:\n",
    "        dist = euclidean_distance(test_row, i)\n",
    "        distance.append(dist)\n",
    "        data.append(i)\n",
    "        \n",
    "    distance = np.array(distance)\n",
    "    data = np.array(data)\n",
    "    # Find the index of min distance and sorting accordingly\n",
    "    index_dist = distance.argsort()\n",
    "    # We arrange our data according to index\n",
    "    data = data[index_dist]\n",
    "    # slicing k neighbors from data[]\n",
    "    neighbors = data[:k]\n",
    "    return neighbors\n",
    "\n",
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    classes = []\n",
    "    for i in neighbors:\n",
    "        classes.append(i[-1])\n",
    "    prediction = max(set(classes), key=classes.count)\n",
    "    return prediction\n",
    "\n",
    "# Converting STRING values to unique int values for each column:\n",
    "# buying:   vhigh, high, med, low. -> (0|2,3,1)\n",
    "# maint:    vhigh, high, med, low. -> (0,2,3,1)\n",
    "# doors:    2, 3, 4, 5more -> (1, 0, 2, 3)\n",
    "# persons:  2, 4, more -> (1, 2, 0)\n",
    "# lug_boot: small, med, big -> (0, 2, 1)\n",
    "# safety:   low, med, high -> (2, 0, 1)\n",
    "new_observation_row = [3, 0, 3, 2, 2, 0]\n",
    "\n",
    "# Large k-value(optional) -> replace it with \n",
    "large_k = int(sqrt(len(dataset)))\n",
    "if large_k%2==1 : large_k+=1\n",
    "\n",
    "prediction = predict_classification(dataset, new_observation_row, 3)\n",
    "prediction_large_k = predict_classification(dataset, new_observation_row, large_k)\n",
    "\n",
    "print('************************ FINAL PREDICTION OUTPUT ************************')\n",
    "print('New Observation:', new_observation_row)\n",
    "\n",
    "class_mapping = {0: 'good', 1: 'vgood', 2: 'acc', 3: 'unacc'}\n",
    "predicted_class = class_mapping[prediction]\n",
    "print('Predicted class(k=3):', predicted_class)\n",
    "\n",
    "predicted_class_large_k = class_mapping[prediction_large_k]\n",
    "print('Predicted class(k=sqrt(dataset_size)):', predicted_class_large_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620cab82-7096-4f03-bbc6-3759b8bcdd5c",
   "metadata": {},
   "source": [
    "## 10 Fold Accuracy for Car_Evaluation(scratch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "719a006c-7beb-42e2-beff-77139a5b410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************ CROSS-VALIDATION RESULTS ************************\n",
      "Accuracy Scores(k=3): [65.69767441860465, 61.04651162790697, 67.44186046511628, 66.27906976744185, 61.627906976744185, 64.53488372093024, 66.86046511627907, 65.69767441860465, 55.81395348837209, 63.95348837209303] \n",
      "\n",
      "Accuracy Scores(k=sqrt(dataset_size)): [69.18604651162791, 69.76744186046511, 72.09302325581395, 70.93023255813954, 70.93023255813954, 70.34883720930233, 75.0, 73.25581395348837, 63.372093023255815, 68.6046511627907] \n",
      "\n",
      "Average Accuracy(k=3): 63.895348837209305\n",
      "Average Accuracy(k=sqrt(dataset_size)): 70.34883720930233\n"
     ]
    }
   ],
   "source": [
    "random.seed(20)\n",
    "n_folds = 10\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    dataset_split = []\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = int(len(dataset) / n_folds)\n",
    "    for _ in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append(dataset_copy.pop(index))\n",
    "        dataset_split.append(fold)\n",
    "    return dataset_split\n",
    "\n",
    "\n",
    "def evaluate_algorithm(dataset, n_folds, k):\n",
    "    folds = cross_validation_split(dataset, n_folds)\n",
    "    scores = []\n",
    "    scores_large_k = []\n",
    "    for fold in folds:\n",
    "        train_set = list(folds)\n",
    "        train_set.remove(fold)\n",
    "        train_set = sum(train_set, [])\n",
    "        test_set = []\n",
    "        for row in fold:\n",
    "            row_copy = list(row)\n",
    "            test_set.append(row_copy)\n",
    "            row_copy[-1] = None  # remove the class label for testing\n",
    "        predictions = []\n",
    "        predictions_large_k = []\n",
    "        for row in test_set:\n",
    "            output = predict_classification(train_set, row, k)\n",
    "            output_large_k = predict_classification(train_set, row, large_k)\n",
    "            predictions.append(output)\n",
    "            predictions_large_k.append(output_large_k)\n",
    "        actual = [row[-1] for row in fold]\n",
    "        correct = sum(1 for i in range(len(actual)) if actual[i] == predictions[i])\n",
    "        correct_large_k = sum(1 for i in range(len(actual)) if actual[i] == predictions_large_k[i])\n",
    "        accuracy = (correct / float(len(actual))) * 100.0\n",
    "        accuracy_large_k = (correct_large_k / float(len(actual))) * 100.0\n",
    "        scores.append(accuracy)\n",
    "        scores_large_k.append(accuracy_large_k)\n",
    "    return [scores, scores_large_k]\n",
    "    \n",
    "k = 3\n",
    "scores, scores_large_k = evaluate_algorithm(dataset, n_folds, k)\n",
    "\n",
    "print('************************ CROSS-VALIDATION RESULTS ************************')\n",
    "print('Accuracy Scores(k=3):', scores, \"\\n\")\n",
    "print('Accuracy Scores(k=sqrt(dataset_size)):', scores_large_k, \"\\n\")\n",
    "print('Average Accuracy(k=3):', np.mean(scores))\n",
    "print('Average Accuracy(k=sqrt(dataset_size)):', np.mean(scores_large_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7372fc-cce8-4ed8-8507-0b8476fe4f20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
